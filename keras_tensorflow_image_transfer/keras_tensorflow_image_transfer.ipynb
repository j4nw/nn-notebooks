{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: [Neural Style Transfer: Creating Art with Deep Learning using tf.keras and eager execution][medium]\n",
    "\n",
    "Original [Github repository][github], licensed under [Apache License 2.0][license]\n",
    "\n",
    "[medium]: https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\n",
    "[github]: https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb\n",
    "[license]: https://apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "from tensorflow.python.keras import models \n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(url):\n",
    "    max_dim = 512\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image.thumbnail((max_dim, max_dim))\n",
    "    return image.copy()\n",
    "\n",
    "def process_image(image):\n",
    "    image = kp_image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return tf.keras.applications.vgg19.preprocess_input(image)\n",
    "\n",
    "def deprocess_image(processed_image):\n",
    "    image = processed_image.copy()\n",
    "    if len(image.shape) == 4:\n",
    "        image = np.squeeze(image, 0)\n",
    "    image[:, :, 0] += 103.939\n",
    "    image[:, :, 1] += 116.779\n",
    "    image[:, :, 2] += 123.68\n",
    "    image = image[:, :, ::-1]\n",
    "    return np.clip(image, 0, 255).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "    content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "    model_outputs = style_outputs + content_outputs\n",
    "    return models.Model(vgg.input, model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content_loss(base_content, target):\n",
    "    return tf.reduce_mean(tf.square(base_content - target))\n",
    "\n",
    "def gram_matrix(input_tensor):\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "    return gram / tf.cast(n, tf.float32)\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_image, style_image):\n",
    "    content_image = process_image(content_image)\n",
    "    style_image = process_image(style_image)\n",
    "    style_outputs = model(style_image)\n",
    "    content_outputs = model(content_image)\n",
    "    style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
    "    content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
    "    \n",
    "    return style_features, content_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "    style_weight, content_weight = loss_weights\n",
    "    model_outputs = model(init_image)\n",
    "    style_output_features = model_outputs[:num_style_layers]\n",
    "    content_output_features = model_outputs[num_style_layers:]\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    \n",
    "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "        style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n",
    "        \n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    \n",
    "    for target_content, comb_content in zip(content_features, content_output_features):\n",
    "        content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n",
    "        \n",
    "    style_score *= style_weight\n",
    "    content_score *= content_weight\n",
    "    loss = style_score + content_score\n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape: \n",
    "        all_loss = compute_loss(**cfg)\n",
    "    total_loss = all_loss[0]\n",
    "    return tape.gradient(total_loss, cfg['init_image']), all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_style_transfer(content_image, style_image, num_iterations, content_weight, style_weight): \n",
    "    model = get_model() \n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    style_features, content_features = get_feature_representations(model, content_image, style_image)\n",
    "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "    init_image = process_image(content_image)\n",
    "    init_image = tfe.Variable(init_image, dtype=tf.float32)\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "    iter_count = 1\n",
    "    best_loss, best_image = float('inf'), None\n",
    "    loss_weights = (style_weight, content_weight)\n",
    "    \n",
    "    cfg = {\n",
    "        'model': model,\n",
    "        'loss_weights': loss_weights,\n",
    "        'init_image': init_image,\n",
    "        'gram_style_features': gram_style_features,\n",
    "        'content_features': content_features\n",
    "    }\n",
    "    \n",
    "    norm_means = np.array([103.939, 116.779, 123.68])\n",
    "    min_vals = -norm_means\n",
    "    max_vals = 255 - norm_means\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, all_loss = compute_grads(cfg)\n",
    "        loss, style_score, content_score = all_loss\n",
    "        opt.apply_gradients([(grads, init_image)])\n",
    "        clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "        init_image.assign(clipped)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_image = deprocess_image(init_image.numpy())\n",
    "            \n",
    "    return best_image, best_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_images = {\n",
    "    'torun': 'https://upload.wikimedia.org/wikipedia/commons/c/cd/Toru%C5%84%2C_Ko%C5%9Bci%C3%B3%C5%82_%C5%9Aw._Ducha.jpg',\n",
    "    'zubr': 'https://upload.wikimedia.org/wikipedia/commons/2/29/Bia%C5%82owieski_Park_Narodowy03_23a.jpg'\n",
    "}\n",
    "\n",
    "style_images = {\n",
    "    'witkacy': 'https://upload.wikimedia.org/wikipedia/commons/2/2e/Witkacy_Kuszenie_sw_A_2.jpg',\n",
    "    'vangogh': 'https://upload.wikimedia.org/wikipedia/commons/f/f2/VangoghStarry-night2.jpg',\n",
    "    'matejko': 'https://upload.wikimedia.org/wikipedia/commons/8/88/Jan_Matejko-Astronomer_Copernicus-Conversation_with_God.jpg'\n",
    "}\n",
    "\n",
    "for name, address in content_images.items():\n",
    "    content_images[name] = load_image(address)\n",
    "\n",
    "for name, address in style_images.items():\n",
    "    style_images[name] = load_image(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_exponents = [1, -1, -3, -4, -5, -6, -9]\n",
    "\n",
    "for content_image_name, content_image in content_images.items():\n",
    "    for style_image_name, style_image in style_images.items():\n",
    "        fig = plt.figure(figsize=(15, 15))\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        ax.imshow(content_image)\n",
    "        ax.set_title('Content image')\n",
    "        ax.axis('off')\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        ax.imshow(style_image)\n",
    "        ax.set_title('Style image')\n",
    "        ax.axis('off')\n",
    "        fig.savefig(f'{content_image_name}_{style_image_name}_originals.png')\n",
    "#         plt.show()\n",
    "        \n",
    "        for e in style_exponents:\n",
    "            content_weight = 1\n",
    "            style_weight = 10 ** e\n",
    "            best_image, best_loss = run_style_transfer(\n",
    "                content_image,\n",
    "                style_image,\n",
    "                content_weight=content_weight,\n",
    "                style_weight=style_weight,\n",
    "                num_iterations=1000\n",
    "            )\n",
    "            best_image = Image.fromarray(best_image)\n",
    "            best_image.save(f'results/{content_image_name}_{style_image_name}_style_1e{e}.png')\n",
    "#             fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#             ax.imshow(best_image)\n",
    "#             ax.set_title(f'Style weight: 1e-{e}')\n",
    "#             ax.axis('off')\n",
    "#             plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
