{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 2 # hidden neuron count\n",
    "X = np.loadtxt('data/mlp_in.txt')\n",
    "y = np.loadtxt('data/mlp_out.txt')\n",
    "learning_rate = 5e-3\n",
    "max_iterations = int(5e5)\n",
    "show_every = int(max_iterations / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x): # sigmoid\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def activation_d(x): # sigmoid derivative\n",
    "    s = activation(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def cost(y1, y2):\n",
    "    return (np.linalg.norm(y1 - y2) ** 2) / 2\n",
    "\n",
    "def cost_d(y1, y2):\n",
    "    return (y1 - y2) * activation_d(y1)\n",
    "\n",
    "def mlp_train(X, y, nh, learning_rate=1e-2, max_iterations=int(1e5), show_every=1000):\n",
    "    ni = 1 if len(X.shape) == 1 else len(X[0]) # input neuron count\n",
    "    no = 1 if len(y.shape) == 1 else len(y[0]) # output neuron count\n",
    "    \n",
    "    hw = np.random.rand(ni, nh) # hidden layer weights\n",
    "    hb = np.random.randn(1, nh) # hidden layer bias\n",
    "    \n",
    "    ow = np.random.rand(nh, no) # output layer weights\n",
    "    ob = np.random.randn(1, no) # output layer bias\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        if (iteration % show_every == 0):\n",
    "            print('iteration', iteration)\n",
    "        for xi, yi in zip(X, y):\n",
    "            xi = xi[np.newaxis, :]\n",
    "            \n",
    "            ha = np.dot(xi, hw) + hb\n",
    "            ho = activation(ha)\n",
    "            \n",
    "            oa = np.dot(ho, ow) + ob\n",
    "            oo = activation(oa)\n",
    "            \n",
    "            c = cost(oo, yi)\n",
    "            \n",
    "            grad_ob = cost_d(oo, yi)\n",
    "            grad_ow = np.dot(ho.T, grad_ob)\n",
    "            \n",
    "            ow -= learning_rate * grad_ow\n",
    "            ob -= learning_rate * grad_ob\n",
    "            \n",
    "            grad_hb = np.dot(grad_ob, ow.T) * (ho * (1 - ho))\n",
    "            grad_hw = np.dot(xi.T, grad_hb)\n",
    "            \n",
    "            hw -= learning_rate * grad_hw\n",
    "            hb -= learning_rate * grad_hb\n",
    "            \n",
    "            if (iteration % show_every == 0):\n",
    "                print(xi, '->', oo, 'cost', c)\n",
    "    \n",
    "    return (hw, ow), (hb, ob)\n",
    "            \n",
    "def mlp_predict(x, w, b):\n",
    "    hw, ow = w\n",
    "    hb, ob = b\n",
    "    ha = np.dot(x, hw) + hb\n",
    "    ho = activation(ha)\n",
    "    oa = np.dot(ho, ow) + ob\n",
    "    oo = activation(oa)\n",
    "    return oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "[[0. 0.]] -> [[0.73529909]] cost 0.2703323752982125\n",
      "[[0. 1.]] -> [[0.75322895]] cost 0.03044797455038804\n",
      "[[1. 0.]] -> [[0.74037906]] cost 0.03370151629866105\n",
      "[[1. 1.]] -> [[0.75762876]] cost 0.28700066837091803\n",
      "iteration 100000\n",
      "[[0. 0.]] -> [[0.29335753]] cost 0.04302931914125131\n",
      "[[0. 1.]] -> [[0.55896428]] cost 0.09725625528425884\n",
      "[[1. 0.]] -> [[0.57330287]] cost 0.09103522169029009\n",
      "[[1. 1.]] -> [[0.5980108]] cost 0.1788084558572554\n",
      "iteration 200000\n",
      "[[0. 0.]] -> [[0.03360377]] cost 0.0005646066517643046\n",
      "[[0. 1.]] -> [[0.9635055]] cost 0.0006659241761070079\n",
      "[[1. 0.]] -> [[0.96344531]] cost 0.0006681225532678748\n",
      "[[1. 1.]] -> [[0.03422717]] cost 0.000585749547101952\n",
      "iteration 300000\n",
      "[[0. 0.]] -> [[0.01450236]] cost 0.00010515919056558344\n",
      "[[0. 1.]] -> [[0.98586714]] cost 9.98688807417933e-05\n",
      "[[1. 0.]] -> [[0.98585201]] cost 0.00010008276610595416\n",
      "[[1. 1.]] -> [[0.01225632]] cost 7.510871123782723e-05\n",
      "iteration 400000\n",
      "[[0. 0.]] -> [[0.00915563]] cost 4.191276066565313e-05\n",
      "[[0. 1.]] -> [[0.99136414]] cost 3.7288997781419675e-05\n",
      "[[1. 0.]] -> [[0.99135696]] cost 3.7351031224523396e-05\n",
      "[[1. 1.]] -> [[0.00730725]] cost 2.6697978762018152e-05\n",
      "final:\n",
      "[0. 0.] -> [[0.00667089]]\n",
      "[0. 1.] -> [[0.9938099]]\n",
      "[1. 0.] -> [[0.99380548]]\n",
      "[1. 1.] -> [[0.00517331]]\n"
     ]
    }
   ],
   "source": [
    "w, b = mlp_train(X, y, nh, learning_rate, max_iterations, show_every)\n",
    "\n",
    "print('final:')\n",
    "for x in X:\n",
    "    o = mlp_predict(x, w, b)\n",
    "    print(x, '->', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer weights:\n",
      "[[7.04839426 5.22942716]\n",
      " [7.05518855 5.23068698]]\n",
      "hidden layer bias:\n",
      "[[-3.1884271  -8.00381815]]\n",
      "output layer weights:\n",
      "[[ 11.48926528]\n",
      " [-12.26235715]]\n",
      "output layer bias:\n",
      "[[-5.45422749]]\n"
     ]
    }
   ],
   "source": [
    "hw, ow = w\n",
    "hb, ob = b\n",
    "print('hidden layer weights:')\n",
    "print(hw)\n",
    "print('hidden layer bias:')\n",
    "print(hb)\n",
    "print('output layer weights:')\n",
    "print(ow)\n",
    "print('output layer bias:')\n",
    "print(ob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
